import torch


def calc_mean_std(feat, eps=1e-5):
    """
    Calculate channel-wise mean and standard deviation for the input features and preserve the dimensions
    Args:
        feat: the latent feature of shape [B, C, H, W]
        eps: a small value to prevent calculation error of variance

    Returns:
    Channel-wise mean and standard deviation of the input features
    """
    size = feat.size()
    assert (len(size) == 4)
    N, C = size[:2]
    feat_var = feat.view(N, C, -1).var(dim=2) + eps
    feat_std = feat_var.sqrt().view(N, C, 1, 1)
    feat_mean = feat.view(N, C, -1).mean(dim=2).view(N, C, 1, 1)
    return feat_mean, feat_std


def calc_feat_mean_std(input, eps=1e-5):
    """
    Calculate channel-wise mean and standard deviation for the input features but reduce the dimensions
    Args:
        input: the latent feature of shape [B, C, H, W]
        eps: a small value to prevent calculation error of variance

    Returns:
    Channel-wise mean and standard deviation of the input features
    """
    size = input.size()
    assert (len(size) == 4)
    N, C = size[:2]
    feat_var = input.view(N, C, -1).var(dim=2) + eps
    feat_std = feat_var.sqrt().view(N, C)
    feat_mean = input.view(N, C, -1).mean(dim=2).view(N, C)
    return torch.cat([feat_mean, feat_std], dim=1)


def adaptive_instance_normalization_with_noise(content_feat, style_feat):
    """
    Implementation of AdaIN of style transfer
    Args:
        content_feat: the content features of shape [B, C, H, W]
        style_feat: the style features of shape [B, C, H, W]

    Returns:
    The re-normalized features
    """
    size = content_feat.size()
    C = size[1]
    N = style_feat.size()[0]
    style_mean = style_feat[:, :512].view(N, C, 1, 1)
    style_std = style_feat[:, 512:].view(N, C, 1, 1)
    content_mean, content_std = calc_mean_std(content_feat)

    normalized_feat = (content_feat - content_mean.expand(
        size)) / content_std.expand(size)
    return normalized_feat * style_std.expand(size) + style_mean.expand(size)


def style_transfer(encoder, decoder, fc_encoder, fc_decoder, content, style, sampling=None):
    """
    RAIN implementation that generate images which preserve content of content images and style of style images
    Args:
        encoder: the VGG encoder
        decoder: the VGG-like decoder
        fc_encoder: the VAE encoder
        fc_decoder: the VAE decoder
        content: the content images
        style: the style images
        sampling: the epsilon sampled from a distribution generated by the VAE encoder
    Returns:
    Images which preserve content of content images and style of style images, the sampling which will be updated for
    the following iterations
    """
    with torch.no_grad():
        content_feat = encoder(content)
        style_feat = encoder(style)
    style_feat_mean_std = calc_feat_mean_std(style_feat)
    intermediate = fc_encoder(style_feat_mean_std)
    intermediate_mean = intermediate[:, :512]
    intermediate_std = intermediate[:, 512:]
    noise = torch.randn_like(intermediate_mean)
    if sampling is None:
        sampling = intermediate_mean + noise * intermediate_std  # N, 512
    # sampling.requires_grad = True
    style_feat_mean_std_recons = fc_decoder(sampling)  # N, 1024
    feat = adaptive_instance_normalization_with_noise(content_feat, style_feat_mean_std_recons)

    return decoder(feat), sampling
